{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import os, random\n",
    "import time\n",
    "from PIL import Image\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from albumentations import Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TIMM_MODEL = 'tf_efficientnet_b3' #b3 imagenet\n",
    "#TIMM_MODEL = 'tf_efficientnet_b4_ns' #b4 noisy student\n",
    "TIMM_MODEL = 'tf_efficientnet_b3_ns' #b3 noisy student \n",
    "class CassavaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = timm.create_model(TIMM_MODEL, pretrained=True)\n",
    "        #n_features = backbone.classifier.in_features\n",
    "        self.backbone = nn.Sequential(*backbone.children())[:-2]\n",
    "        #self.classifier = nn.Linear(1536, 5)\n",
    "        self.classifier = nn.Sequential(backbone.classifier)\n",
    "        self.classifier.add_module('7', nn.ReLU())\n",
    "        self.classifier.add_module('8', nn.Dropout(p=0.5, inplace=False))\n",
    "        self.classifier.add_module('9', nn.Linear(1000, 5))\n",
    "        #self.classifier.add_module('10', nn.LogSoftmax(dim=1))\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.forward_features(x)\n",
    "        x = self.pool(feats).view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x, feats\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CassavaNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2dSame(3, 40, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU(inplace=True)\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (bn1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pw): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(192, 192, kernel_size=(5, 5), stride=(2, 2), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(288, 288, kernel_size=(3, 3), stride=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(816, 816, kernel_size=(5, 5), stride=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "          (bn2): BatchNorm2d(2304, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (conv_pwl): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): SiLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=1000, out_features=5, bias=True)\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CassavaNet()\n",
    "\n",
    "for module in model.modules():\n",
    "    # print(module)\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        if hasattr(module, 'weight'):\n",
    "            module.weight.requires_grad_(False)\n",
    "        if hasattr(module, 'bias'):\n",
    "            module.bias.requires_grad_(False)\n",
    "        module.eval()\n",
    "            \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Cassava dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (string): dataframe train/valid\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get_img_bgr_to_rgb(self, path):\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        return im_rgb\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.dataframe.iloc[idx, 0])\n",
    "        image = self.get_img_bgr_to_rgb(img_name)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        csv_row = self.dataframe.iloc[idx, 1:]\n",
    "        sample = {\n",
    "            'image': image, \n",
    "            'label': csv_row.label,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/'\n",
    "NUM_FOLDS = 5\n",
    "bs = 4\n",
    "# Running only 5 epochs to test (Train more offline ^_^)\n",
    "EPOCHS = 30\n",
    "sz = 512\n",
    "SNAPMIX_ALPHA = 5.0\n",
    "SNAPMIX_PCT = 0.\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "\n",
    "#norm try with mean=[0.4580, 0.5274, 0.3245], std=[0.2267, 0.2285, 0.2170] too\n",
    "def train_transforms():\n",
    "    return Compose([\n",
    "            A.RandomResizedCrop(sz, sz),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.ShiftScaleRotate(p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "\n",
    "def valid_transforms():\n",
    "    return Compose([\n",
    "            A.Resize(sz, sz),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 1234\n",
    "seed_everything(SEED)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def get_spm(input,target,model):\n",
    "    imgsize = (sz, sz)\n",
    "    bs = input.size(0)\n",
    "    with torch.no_grad():\n",
    "        output,fms = model(input)\n",
    "        clsw = model.classifier\n",
    "        weight = clsw[-1].weight.data\n",
    "        bias = clsw[-1].bias.data\n",
    "        weight = weight.view(weight.size(0),weight.size(1),1,1)\n",
    "        fms = F.relu(fms)\n",
    "        poolfea = F.adaptive_avg_pool2d(fms,(1,1)).squeeze()\n",
    "        clslogit = F.softmax(clsw.forward(poolfea))\n",
    "        logitlist = []\n",
    "        for i in range(bs):\n",
    "            logitlist.append(clslogit[i,target[i]])\n",
    "        clslogit = torch.stack(logitlist)\n",
    "\n",
    "        out = F.conv2d(fms, weight, bias=bias)\n",
    "\n",
    "        outmaps = []\n",
    "        for i in range(bs):\n",
    "            evimap = out[i,target[i]]\n",
    "            outmaps.append(evimap)\n",
    "\n",
    "        outmaps = torch.stack(outmaps)\n",
    "        if imgsize is not None:\n",
    "            outmaps = outmaps.view(outmaps.size(0),1,outmaps.size(1),outmaps.size(2))\n",
    "            outmaps = F.interpolate(outmaps,imgsize,mode='bilinear',align_corners=False)\n",
    "\n",
    "        outmaps = outmaps.squeeze()\n",
    "\n",
    "        for i in range(bs):\n",
    "            outmaps[i] -= outmaps[i].min()\n",
    "            outmaps[i] /= outmaps[i].sum()\n",
    "\n",
    "\n",
    "    return outmaps,clslogit\n",
    "\n",
    "def snapmix(input, target, alpha, model=None):\n",
    "\n",
    "    r = np.random.rand(1)\n",
    "    lam_a = torch.ones(input.size(0))\n",
    "    lam_b = 1 - lam_a\n",
    "    target_b = target.clone()\n",
    "\n",
    "    if True:\n",
    "        wfmaps,_ = get_spm(input, target, model)\n",
    "        bs = input.size(0)\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam1 = np.random.beta(alpha, alpha)\n",
    "        rand_index = torch.randperm(bs).cuda()\n",
    "        wfmaps_b = wfmaps[rand_index,:,:]\n",
    "        target_b = target[rand_index]\n",
    "\n",
    "        same_label = target == target_b\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        bbx1_1, bby1_1, bbx2_1, bby2_1 = rand_bbox(input.size(), lam1)\n",
    "\n",
    "        area = (bby2-bby1)*(bbx2-bbx1)\n",
    "        area1 = (bby2_1-bby1_1)*(bbx2_1-bbx1_1)\n",
    "\n",
    "        if  area1 > 0 and  area>0:\n",
    "            ncont = input[rand_index, :, bbx1_1:bbx2_1, bby1_1:bby2_1].clone()\n",
    "            ncont = F.interpolate(ncont, size=(bbx2-bbx1,bby2-bby1), mode='bilinear', align_corners=True)\n",
    "            input[:, :, bbx1:bbx2, bby1:bby2] = ncont\n",
    "            lam_a = 1 - wfmaps[:,bbx1:bbx2,bby1:bby2].sum(2).sum(1)/(wfmaps.sum(2).sum(1)+1e-8)\n",
    "            lam_b = wfmaps_b[:,bbx1_1:bbx2_1,bby1_1:bby2_1].sum(2).sum(1)/(wfmaps_b.sum(2).sum(1)+1e-8)\n",
    "            tmp = lam_a.clone()\n",
    "            lam_a[same_label] += lam_b[same_label]\n",
    "            lam_b[same_label] += tmp[same_label]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "            lam_a[torch.isnan(lam_a)] = lam\n",
    "            lam_b[torch.isnan(lam_b)] = 1-lam\n",
    "\n",
    "    return input,target,target_b,lam_a.cuda(),lam_b.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(input, targs):\n",
    "    return accuracy_score(targs.cpu(), input.cpu())\n",
    "\n",
    "def print_scores(scores):\n",
    "    kaggle_metric = np.average(scores)\n",
    "    print(\"Kaggle Metric: %f\" % (kaggle_metric))\n",
    "    \n",
    "    return kaggle_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, optimizer, epoch, current_metric, best_metric, fold):\n",
    "    print(\"Metric improved from %f to %f , Saving Model at Epoch #%d\" % (best_metric, current_metric, epoch))\n",
    "    ckpt = {\n",
    "        'model': CassavaNet(),\n",
    "        'state_dict': model.state_dict(),\n",
    "        #'optimizer' : optimizer.state_dict(),  # Commenting this out to cheap out on space\n",
    "        'metric': current_metric\n",
    "    }\n",
    "    torch.save(ckpt, '/media/sergio/traballos sergio/msi/models/cassava/ckpt_%s-%d-%f-snap=0-extra.pth' % (TIMM_MODEL, sz, current_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u'.\"\"\"\n",
    "    if t==1.0:\n",
    "        return u.log()\n",
    "    else:\n",
    "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
    "    if t==1:\n",
    "        return u.exp()\n",
    "    else:\n",
    "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = torch.sum(\n",
    "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "        normalized_activations = normalized_activations_step_0 * \\\n",
    "                logt_partition.pow(1.0-t)\n",
    "\n",
    "    logt_partition = torch.sum(\n",
    "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
    "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "    return normalization_constants\n",
    "\n",
    "def compute_normalization_binary_search(activations, t, num_iters):\n",
    "\n",
    "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (< 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
    "    normalized_activations = activations - mu\n",
    "\n",
    "    effective_dim = \\\n",
    "        torch.sum(\n",
    "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
    "            dim=-1, keepdim=True).to(activations.dtype)\n",
    "\n",
    "    shape_partition = activations.shape[:-1] + (1,)\n",
    "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
    "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        logt_partition = (upper + lower)/2.0\n",
    "        sum_probs = torch.sum(\n",
    "                exp_t(normalized_activations - logt_partition, t),\n",
    "                dim=-1, keepdim=True)\n",
    "        update = (sum_probs < 1.0).to(activations.dtype)\n",
    "        lower = torch.reshape(\n",
    "                lower * update + (1.0-update) * logt_partition,\n",
    "                shape_partition)\n",
    "        upper = torch.reshape(\n",
    "                upper * (1.0 - update) + update * logt_partition,\n",
    "                shape_partition)\n",
    "\n",
    "    logt_partition = (upper + lower)/2.0\n",
    "    return logt_partition + mu\n",
    "\n",
    "class ComputeNormalization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, activations, t, num_iters):\n",
    "        if t < 1.0:\n",
    "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
    "        else:\n",
    "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "        ctx.save_for_backward(activations, normalization_constants)\n",
    "        ctx.t=t\n",
    "        return normalization_constants\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        activations, normalization_constants = ctx.saved_tensors\n",
    "        t = ctx.t\n",
    "        normalized_activations = activations - normalization_constants \n",
    "        probabilities = exp_t(normalized_activations, t)\n",
    "        escorts = probabilities.pow(t)\n",
    "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
    "        grad_input = escorts * grad_output\n",
    "        \n",
    "        return grad_input, None, None\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example. \n",
    "    Backward pass is implemented.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "    return ComputeNormalization.apply(activations, t, num_iters)\n",
    "\n",
    "def tempered_sigmoid(activations, t, num_iters = 5):\n",
    "    \"\"\"Tempered sigmoid function.\n",
    "    Args:\n",
    "      activations: Activations for the positive class for binary classification.\n",
    "      t: Temperature tensor > 0.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
    "    return internal_probabilities[..., 0]\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      t: Temperature > 1.0.\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A probabilities tensor.\n",
    "    \"\"\"\n",
    "    if t == 1.0:\n",
    "        return activations.softmax(dim=-1)\n",
    "\n",
    "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "def bi_tempered_binary_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing = 0.0,\n",
    "        num_iters=5,\n",
    "        reduction='mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered binary logistic loss.\n",
    "    Args:\n",
    "      activations: A tensor containing activations for class 1.\n",
    "      labels: A tensor with shape as activations, containing probabilities for class 1\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing\n",
    "      num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "    internal_activations = torch.stack([activations,\n",
    "        torch.zeros_like(activations)],\n",
    "        dim=-1)\n",
    "    internal_labels = torch.stack([labels.to(activations.dtype),\n",
    "        1.0 - labels.to(activations.dtype)],\n",
    "        dim=-1)\n",
    "    return bi_tempered_logistic_loss(internal_activations, \n",
    "            internal_labels,\n",
    "            t1,\n",
    "            t2,\n",
    "            label_smoothing = label_smoothing,\n",
    "            num_iters = num_iters,\n",
    "            reduction = reduction)\n",
    "\n",
    "def bi_tempered_logistic_loss(activations,\n",
    "        labels,\n",
    "        t1,\n",
    "        t2,\n",
    "        label_smoothing=0.0,\n",
    "        num_iters=5,\n",
    "        reduction = 'mean'):\n",
    "\n",
    "    \"\"\"Bi-Tempered Logistic Loss.\n",
    "    Args:\n",
    "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "      labels: A tensor with shape and dtype as activations (onehot), \n",
    "        or a long tensor of one dimension less than activations (pytorch standard)\n",
    "      t1: Temperature 1 (< 1.0 for boundedness).\n",
    "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
    "      num_iters: Number of iterations to run the method. Default 5.\n",
    "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
    "        ``'none'``: No reduction is applied, return shape is shape of\n",
    "        activations without the last dimension.\n",
    "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
    "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
    "    Returns:\n",
    "      A loss tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
    "        labels_onehot = torch.zeros_like(activations)\n",
    "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
    "    else:\n",
    "        labels_onehot = labels\n",
    "\n",
    "    if label_smoothing > 0:\n",
    "        num_classes = labels_onehot.shape[-1]\n",
    "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
    "                * labels_onehot + \\\n",
    "                label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
    "            - labels_onehot * log_t(probabilities, t1) \\\n",
    "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
    "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
    "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss_values\n",
    "    if reduction == 'sum':\n",
    "        return loss_values.sum()\n",
    "    if reduction == 'mean':\n",
    "        return loss_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapMixLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, t1, t2, ls,  ya, yb, lam_a, lam_b):\n",
    "        loss_a = bi_tempered_logistic_loss(outputs, ya, t1, t2, ls)\n",
    "        loss_b = bi_tempered_logistic_loss(outputs, yb, t1, t2, ls)\n",
    "        loss = torch.mean(loss_a * lam_a + loss_b * lam_b)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All data\n",
    "train_df = pd.read_csv(DATA_PATH + \"train_extra.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21392</th>\n",
       "      <td>999068805.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21393</th>\n",
       "      <td>999329392.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21394</th>\n",
       "      <td>999474432.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21395</th>\n",
       "      <td>999616605.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21396</th>\n",
       "      <td>999998473.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21397 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  label\n",
       "0      1000015157.jpg      0\n",
       "1      1000201771.jpg      3\n",
       "2       100042118.jpg      1\n",
       "3      1000723321.jpg      1\n",
       "4      1000812911.jpg      3\n",
       "...               ...    ...\n",
       "21392   999068805.jpg      3\n",
       "21393   999329392.jpg      3\n",
       "21394   999474432.jpg      1\n",
       "21395   999616605.jpg      4\n",
       "21396   999998473.jpg      4\n",
       "\n",
       "[21397 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial data\n",
    "train_df = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = cv2.meanStdDev(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, \n",
    "                        random_state=SEED).split(np.arange(train_df.shape[0]), \n",
    "                                                 train_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 1 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'train_loss': 0.6676}: 100%|| 5410/5410 [15:34<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-15-bd041b10f901>:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = F.softmax(outputs).argmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'train_loss': 0.6676, 'val_loss': 0.4858}\n",
      "Kaggle Metric: 0.820030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'train_loss': 0.4691}: 100%|| 5410/5410 [16:07<00:00,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 1 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'train_loss': 0.4691, 'val_loss': 0.4471}\n",
      "Kaggle Metric: 0.839985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'train_loss': 0.4369}:  71%|  | 3861/5410 [11:28<05:09,  5.01it/s]"
     ]
    }
   ],
   "source": [
    "for fold_num, (train_split, valid_split) in enumerate(folds):\n",
    "    train_set = train_df.iloc[train_split].reset_index(drop=True)\n",
    "    valid_set = train_df.iloc[valid_split].reset_index(drop=True)\n",
    "    \n",
    "    train_ds = CassavaDataset(dataframe=train_set,\n",
    "                          root_dir='/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/',\n",
    "                          transforms=train_transforms())\n",
    "    \n",
    "    valid_ds = CassavaDataset(dataframe=valid_set,\n",
    "                          root_dir='/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/',\n",
    "                          transforms=valid_transforms())\n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bs, \n",
    "                                           shuffle=True, num_workers=8, drop_last=True,\n",
    "                                           pin_memory=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=4, \n",
    "                                           shuffle=False, num_workers=8, \n",
    "                                           pin_memory=True)\n",
    "    \n",
    "    losses = []\n",
    "    batches = len(train_dl)\n",
    "    val_batches = len(valid_dl)\n",
    "    best_metric = 0\n",
    "    \n",
    "    #model = CassavaNet().to(device)\n",
    "    t1 = 1.0\n",
    "    t2 = 1.0\n",
    "    ls = 0.05\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "    #criterion = bi_tempered_logistic_loss()\n",
    "    #val_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #val_criterion = bi_tempered_logistic_loss()\n",
    "    snapmix_criterion = SnapMixLoss().to(device)\n",
    "    param_groups = [\n",
    "       {'params': model.backbone.parameters(), 'lr': 0.1e-2},\n",
    "       {'params': model.classifier.parameters()},\n",
    "    ]\n",
    "    optimizer = torch.optim.SGD(param_groups, lr=0.1e-1, momentum=0.9,\n",
    "                                weight_decay=1e-4, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1,20,40], \n",
    "                                                     gamma=0.1, last_epoch=-1, verbose=True)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # ----------------- TRAINING  ----------------- \n",
    "        train_loss = 0\n",
    "        progress = tqdm(enumerate(train_dl), desc=\"Loss: \", total=batches)\n",
    "\n",
    "        model.train()\n",
    "        for i, data in progress:\n",
    "            image, label = data.values()\n",
    "            X, y = image.to(device).float(), label.to(device).long()\n",
    "            \n",
    "            with autocast():\n",
    "                \n",
    "                rand = np.random.rand()\n",
    "                if rand > (1.0-SNAPMIX_PCT):\n",
    "                    X, ya, yb, lam_a, lam_b = snapmix(X, y, SNAPMIX_ALPHA, model)\n",
    "                    outputs, _ = model(X)\n",
    "                    loss = snapmix_criterion(outputs, t1, t2, ls, ya, yb, lam_a, lam_b).to(device)\n",
    "                else:\n",
    "                    outputs, _ = model(X)\n",
    "                    loss = torch.mean(bi_tempered_logistic_loss(outputs, y, t1, t2, ls).to(device))\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            # Accumulate gradients\n",
    "            if ((i + 1) % GRAD_ACCUM_STEPS == 0) or ((i + 1) == len(train_dl)):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            cur_step = i+1\n",
    "            trn_epoch_result = dict()\n",
    "            trn_epoch_result['Epoch'] = epoch + 1\n",
    "            trn_epoch_result['train_loss'] = round(train_loss/cur_step, 4)\n",
    "\n",
    "            progress.set_description(str(trn_epoch_result))\n",
    "\n",
    "        scheduler.step()\n",
    "        #if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ----------------- VALIDATION  ----------------- \n",
    "        val_loss = 0\n",
    "        scores = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valid_dl):\n",
    "                image, label = data.values()\n",
    "                X, y = image.to(device), label.to(device)\n",
    "                outputs, _ = model(X)\n",
    "                l = bi_tempered_logistic_loss(outputs, y, t1, t2, ls).to(device)\n",
    "                val_loss += l.item()\n",
    "\n",
    "                preds = F.softmax(outputs).argmax(axis=1)\n",
    "                scores.append(accuracy_metric(preds, y))\n",
    "\n",
    "        epoch_result = dict()\n",
    "        epoch_result['Epoch'] = epoch + 1\n",
    "        epoch_result['train_loss'] = round(train_loss/batches, 4)\n",
    "        epoch_result['val_loss'] = round(val_loss/val_batches, 4)\n",
    "\n",
    "        print(epoch_result)\n",
    "\n",
    "        # Check if we need to save\n",
    "        current_metric = print_scores(scores)\n",
    "        if current_metric > best_metric and current_metric > 0.85:\n",
    "            checkpoint(model, optimizer, epoch+1, current_metric, best_metric, fold_num)\n",
    "            best_metric = current_metric\n",
    "            \n",
    "    del model, optimizer, train_dl, valid_dl, scaler, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train only a single fold\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21397, 21397)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH +'train.csv')\n",
    "old_imgs = [f'train_images/{img}' for img in train['image_id'].values]\n",
    "old_labs = train['label'].tolist()\n",
    "\n",
    "len(old_imgs), len(old_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5656, 5656)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_labels = os.listdir('/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/extra/train')\n",
    "map2labs = {\n",
    "    'cbb': 0,\n",
    "    'cbsd': 1,\n",
    "    'cgm': 2,\n",
    "    'cmd': 3, \n",
    "    'healthy': 4\n",
    "}\n",
    "new_imgs, new_labs = [] ,[]\n",
    "for lab in extra_labels:\n",
    "    imgs = os.listdir('/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/extra/train/'+ lab)\n",
    "    new_imgs += [f'extra/train/{lab}/{img}' for img in imgs]\n",
    "    new_labs += [map2labs[lab]]*len(imgs)\n",
    "    \n",
    "len(new_imgs), len(new_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra = pd.DataFrame({\n",
    "    'image_id': old_imgs + new_imgs,\n",
    "    'label': old_labs + new_labs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_images/1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_images/1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_images/100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_images/1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_images/1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>extra/train/healthy/train-healthy-22.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27049</th>\n",
       "      <td>extra/train/healthy/train-healthy-220.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27050</th>\n",
       "      <td>extra/train/healthy/train-healthy-221.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27051</th>\n",
       "      <td>extra/train/healthy/train-healthy-222.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27052</th>\n",
       "      <td>extra/train/healthy/train-healthy-223.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27053 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        image_id  label\n",
       "0                    train_images/1000015157.jpg      0\n",
       "1                    train_images/1000201771.jpg      3\n",
       "2                     train_images/100042118.jpg      1\n",
       "3                    train_images/1000723321.jpg      1\n",
       "4                    train_images/1000812911.jpg      3\n",
       "...                                          ...    ...\n",
       "27048   extra/train/healthy/train-healthy-22.jpg      4\n",
       "27049  extra/train/healthy/train-healthy-220.jpg      4\n",
       "27050  extra/train/healthy/train-healthy-221.jpg      4\n",
       "27051  extra/train/healthy/train-healthy-222.jpg      4\n",
       "27052  extra/train/healthy/train-healthy-223.jpg      4\n",
       "\n",
       "[27053 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_extra\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(DATA_PATH +'train_extra.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_base_path = '/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/'\n",
    "n_classes = 5\n",
    "files_path = f'{database_base_path}test_images/'\n",
    "test_size = len(os.listdir(files_path))\n",
    "test_preds = np.zeros((test_size, n_classes))\n",
    "loader = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(512),\n",
    "        torchvision.transforms.CenterCrop(448),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "image_names =  os.listdir(files_path)\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU\n",
    "\n",
    "def prediction(model, files_path):\n",
    "    labels = []\n",
    "    for img in os.listdir(files_path):\n",
    "        image = image_loader(files_path + img)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, _ = model(image.cuda())\n",
    "            print(pred.shape)\n",
    "            label = torch.argmax(pred)\n",
    "            labels.append(label.item())\n",
    "    return np.array(labels) \n",
    "\n",
    "test_preds = prediction(model, files_path)\n",
    "test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3db12c4a30>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnsklEQVR4nO3deXhU5d3/8feXLISENQSQLew7gmDEFTdAccW1RWurjyjairXYzbY+rfVpf21txb0qUqq1rVZxQ8WyK24oQRAEJYSwBZR9Tcg69++PM5AhJjKBmTmTyed1XbmYOcvMNyeHT+7c555zm3MOERFJXI38LkBERKJLQS8ikuAU9CIiCU5BLyKS4BT0IiIJLtnvAqrLyspyXbt29bsMEZF6ZfHixdudc21qWhd3Qd+1a1dyc3P9LkNEpF4xs/W1rVPXjYhIglPQi4gkOAW9iEiCU9CLiCQ4Bb2ISIJT0IuIJDgFvYhIgou7cfQiIomsvDLA7uJydhWXsbOojF1FZews9v7tmpXBxYM6RPw9FfQiIkepMuDYXVzGrlqCe2dRyPLgsr0lFbW+3iWDOyjoRUSiJRBw7C0pPxTKB0O6puA+uGzPgXJqm7upSUoSmRmptExPITMjlezMdDIzUmmVnkpmRgqtMlLJTE+lVXBZy/QU0lKSovK9KehFJOE459hXWhEM55DgDgltL7DLD3seqCW0U5MaeSGd4YV0/w7NgyGeSmZ6MLQPhbj3b5PU6IT20VDQi0hcc85RVFZ5KIxrC+5D3SPF3vKKWlI7uZGFtKZT6N2u6WEBfTDQW6WnHHqenpqEmcX4O48cBb2IxNSBssqvh3NRGTuLawnuonLKKgM1vlZSIzsUyK0yUumWlcGJ1VrWh1riwWBv2ji5Xof20VDQi8hRKymvZHdxaL922WEXJWu6SFlSXnNom0HLJlV9150z0xncqeWh7hKvmyT1UDdJZnoqzdKSadSoYYX20VDQiwjgDfs71G9dLbB3Vmt1H2yFF5VV1vp6LZqkHLoYeVzzNPq1b374xcjDuklSadEkhSSFdlQo6EUSUEVlgN0HykO6QGoZ/ncwuIvK2Fda+7C/Zo2TvUDOSCWraSq9vtavfXhwt2ySQnKSPo8ZLxT0InEuEHDsOVD+tb7rnUXl7K7eXRLsRtlzoLzW10tPTToslLu1Tj9sqF/okMDMdG9kSWqyQrs+U9CLxJBzjr0lFTVedAwdRbI7JLh3f8Owv8bJjWidURXQHVul1zrc72CAR2ustsQvBb3IUXLOsb+0omos9jcE96F+7eJyKmtJ7dSkRrQK6QLp2755VSs7GN7VR5HE01htiV8KepEw5G/dx5R317J+R/Fho0vKK795rPbBoX892zat1j2Sclhru1VGKhn1fKy2xC8Fvcg3WLe9iIfmrua1pZtIS0liYIcWdGmdzgmdW9Ye3BmpNGuAY7UlfinoRWpQuKuYR+bmM+2TQlKSjJuHd+eWs3qQmZHqd2kidaagFwnx1Z4SHp2/mv8s2ohhfPeULvzgnB60bZbmd2kiR01BLwJs21fK42+v4Z8frScQcHz7pM5MOLcn7Vs08bs0kWOmoJcGbVdRGU8uKOCZD9ZRVhngiiEd+eGIXnTOTPe7NJGIUdBLg7TnQDl/e7eAqe+vo6isgksHd+COEb3o3qap36WJRFxYQW9mo4GHgCRginPuj9XWZwPPAC2D29zlnJsRXDcIeBJoDgSAk5xzJZH6BkTqYn9pBU+/v5bJCwrYW1LBhccfx49G9qZ3u2Z+lyYSNUcMejNLAh4DRgGFwCIzm+6cWxmy2d3AC865x82sPzAD6GpmycA/ge865z41s9ZA7Z/NFomSA2WVPLtwHU+8U8DOojJG9mvHxFG9GNChhd+liURdOC36YUC+c64AwMyeB8YAoUHv8FrsAC2AzcHH5wHLnHOfAjjndkSiaJFwlZRX8tzHG/jr22vYtq+UM3u34c5RvTmhc0u/SxOJmXCCviOwMeR5IXBytW3uAWaZ2e1ABjAyuLw34MxsJtAGeN45d1/1NzCz8cB4gOzs7LrUL1KjsooALy7eyKPz8vlyTwmndM/kr98ZykldM/0uTSTmInUx9hrgaefc/WZ2KvCsmQ0Mvv4ZwElAMTDXzBY75+aG7uycmwxMBsjJyanl9k0iR1ZRGeDlJZt4eO5qCncd4MQurbj/6sGc1jPL79JEfBNO0G8COoc87xRcFmocMBrAOfehmaUBWXit/wXOue0AZjYDGArMRSSCKgOON5Zt5sE5q1m7vYjjO7bg/y4byNm92+hWBNLghRP0i4BeZtYNL+DHAtdW22YDMAJ42sz6AWnANmAm8DMzSwfKgLOAByJUuwiBgGPmiq+YNDuP1Vv30/e4Zkz+7omM6t9OAS8SdMSgd85VmNkEvNBOAqY651aY2b1ArnNuOvBj4Ckzm4h3YfYG55wDdpnZJLxfFg6Y4Zx7M1rfjDQczjnmfr6VSbPzWPnlXnq0yeDRa4dw4cD2mkNUpBrz8jh+5OTkuNzcXL/LkDjlnOPd1du5f3Yen27cTZfW6dwxohdjTuio+UalQQte/8ypaZ0+GSv1xsKCHdw/axWL1u2iY8sm/OnK47liaCdSNDepyDdS0EvcW7x+F5Nmr+L9/B20a96Y/xszgG+d1JnGyZpdSSQcCnqJW8sL9zBp9irmr9pGVtNU7r6oH9ed0kVznorUkYJe4s4XX+1l0qw8Zq3cQsv0FH4+ui/Xn9aF9FSdriJHQ/9zJG7kb93Pg3PyeHP5lzRNTWbiyN7ceEZXmqWl+F2aSL2moBffrd/hzcv66hJvXtbbzu7JzcO70yJdAS8SCQp68U3hrmIenZfPi4u9eVlvGt6dW87sTuumjf0uTSShKOgl5rbsLeHRefk8v2hD1bysZ/egbXPNyyoSDQp6iZnt+4Pzsi5cT2XA8a2TOjPhnJ50aKl5WUWiSUEvUberqIzJ7xbw9PvrKK2o5Iqhnfjhub3Ibq15WUViQUEvUbO3pJwp765l6ntrKSqr4JJBHbhjZC96aF5WkZhS0EvEFZVW8PQH65i8oIA9B8q5YKA3L2uf4zQvq4gfFPQSMQfKKvnnwvU8/s4adhaVMaJvWyaO6s3AjpqXVcRPCno5ZqUVlTz30QYeC87LOrxXFneO6s2Q7FZ+lyYiKOjlGJRXBngxt5BH561m854STu6WyWPXDmVYN83LKhJPFPRSZxWVAV5dupmH5uaxcecBhma35M9XD+a0Hq01q5NIHFLQS9gCAcfryzbz0JzVFGwvYmDH5tx7w0DO7qN5WUXimYJejsg5b17WB2avZtWWffRp14wnv3si52leVpF6QUEvtXLOMX/VVu6flceKzXvp3iaDR64ZwkXHa15WkfpEQS9f45zjvfzt3D8rj6Ubd5Odmc79Vw9mzAkdSNa0fSL1joJeDvNRwQ7un5XHx+t20qFFGn+84niuPFHzsorUZwp6AeCTDbuYNCuP9/K307ZZY+4dM4Bva15WkYSgoG/gPtu0h0mz85j3xVZaZ2heVpFEpKBvoL74ai8PzM5j5oottGiSws9G9+H6U7uS0VinhEii0f/qBmbNtv08OGc1byzbTNPUZH40shc3ntGN5pqXVSRhKegbiA07inlo7mpeWVJIWkoSPzi7BzcP707L9FS/SxORKFPQJ7hNuw/w6LzVvJhbSFIjY9wZ3bj1rB6al1WkAVHQJ6ite0t4bH4+z328EYDvnJzNbef01LysIg2Qgj7B7NhfyhPvrOEfH3rzsl6d05kJ5/ako+ZlFWmwwgp6MxsNPAQkAVOcc3+stj4beAZoGdzmLufcjGrrVwL3OOf+EpnSJdTu4jImLyjg6Q/WUVJeyeVDOnHHCM3LKhLXnIPd62HzUvhyKWR2h6Hfi/jbHDHozSwJeAwYBRQCi8xsunNuZchmdwMvOOceN7P+wAyga8j6ScBbEataDtlbUs7U99byt3fXsr+sgosHdeBHmpdVJP44B7vWeYF+MNi//BQO7PLWN0qGIdf5E/TAMCDfOVcAYGbPA2PwWugHOaB58HELYPPBFWZ2GbAWKIpAvRJUfV7W0QOOY+IozcsqEhecg11rqwJ981Iv1Et2e+sbpUDbftDvEmh/AnQ4AdoOgJToXEMLJ+g7AhtDnhcCJ1fb5h5glpndDmQAIwHMrCnwc7y/Bn5S2xuY2XhgPEB2dnaYpTdMJeXBeVnfXsOOojLO7duWOzUvq4h/nIOdBV9vqZfs8dY3SoF2/aH/GC/Q258A7QZAcuxGvkXqYuw1wNPOufvN7FTgWTMbiPcL4AHn3P5vum+5c24yMBkgJyfHRaimhFJaUcnzH2/ksfn5bN1Xyhk9s7jzvN4M1bysIrETCARb6ktCgn0ZlAZDPSkV2vaHAZeHtNT7xzTUaxJO0G8COoc87xRcFmocMBrAOfehmaUBWXgt/6vM7D68C7UBMytxzj16rIU3FOWVAaYtLuSRud68rMO6ZfLINUM4uXtrv0sTSWyhob55iddKrx7q7QbAwCuqWupt+0Ny/H0IMZygXwT0MrNueAE/Fri22jYbgBHA02bWD0gDtjnnhh/cwMzuAfYr5MNTGXC8umQTD81dzYadxQzJbsl9Vw3m9J6al1Uk4gKBkO6Xg6H+KZTu9dYnNfZC/fgrq1rqbfrFZajX5IhB75yrMLMJwEy8oZNTnXMrzOxeINc5Nx34MfCUmU3EuzB7g3NOXTBHIRBwvLH8Sx6ck0fBtiIGdGjO1BtyOKdPWwW8SCQEArBzzdcvlJbt89YnNYbjBsLxV4e01PtBUv29H5TFWx7n5OS43Nxcv8uIOW9e1i08OCePL77y5mWdOKo35w/QvKwiRy0QgB351S6ULqsK9eQ0aDewKtA7nABt+tbLUDezxc65nJrW6ZOxPnPO8faqbdw/exWfbfLmZX34miFcrHlZReomUOmFemhL/atlULbfW38w1Ad/GzoM8YK9TZ96Gep1paD3iXOO9/N3cP/sVSzZsJvOmU34y9WDuUzzsoocWaAStq/+eku9PPhxneQmXvfL4GuqWutt+kJSw4y8hvld++zjtTu5f9YqPlrrzcv6hyuO5yrNyypSs0AlbM+r1lJfXi3Uj4ch36nqfsnq02BDvSY6EjG0ZMMuJs3O493V22nTrDG/vXQAY4dpXlaRQyorvFAPbal/tRzKi731KenBUL+uqqWe1VuhfgQ6OjHw2aY9PDA7j7lfbCUzI5VfXejNy9okVQEvDdhhob6kqqVeccBbn5IOxw3y7v1yqKXeGxrp/01dKeijKG/LPh6Yncdbn31FiyYp/PT8PtxwmuZllQaosgK2r/p698uhUM+A9oPgxBtCWuq9FOoRosSJgoLgvKyvL9tMRmoyd4zoxbjhmpdVGojKCtj2RbXul8+qQj21qddSz/mfqpZ6654K9ShS0EfQxp3evKwvf1JI4+Qkbj2rB+OHd6dVRv349JxInVWWe6Ee2lLf8hlUlHjrU5tC+8GQc2NVS711D4V6jCnoI2Dz7gM8Mi+fF3M3ktTIuPH0btx6dg+yNC+rJJLKctj6+ddb6pWl3vrUZl6on3RTVUs9swc00mgyvynoj8HWvSX89e01/PujDTgc1wbnZW2neVmlvqsog22fV2upr6gK9cbNvVAfdnPVh48yuyvU45SC/ijs2F/KkwsK+MeH6yivdHwrpxMTzu2leVmlfqoog60rD2+pb1kBlWXe+uqh3mEItOqmUK9HFPR1sKe4nMnvruHv73vzsl42pCN3jOhFl9YZfpcmEp7DQj04pHHrypBQb+GNfjn5lmD3i0I9ESjow7CvpJyp761jynsF7Cup4OJB7fnRyN70bKt5WSWOVZR6IR7a/RIa6mktvJb6ybdWXShVqCckBf03KC6r4JkP1vPkgjXsLi7n/AHtmDiqN32Pa37knUViqaLU6245rPtlJQTKvfVpLbwgP+X7VRdKW3UD3Rm1QVDQ1+DgvKxPvLOG7fvLOKdPG+4c1YfjO2leVokD5SWwdUW1lvrnIaHe0gvyU28Laal3Vag3YAr6EKUVlbywaCOPzs9ny95STu/ZmidH9eHELpqXVXxSXhJsqS+pCvatn0OgwlvfpJUX5KdNqGqpt+yiUJfDKOjx5mV9+ZNCHp6bz6bdBxjWNZOHxg7hFM3LKrFUfsAL9UMTT3/qDXE8FOqZXpCfNqqqpd4yW6EuR9Sgg74y4HhtqTcv6/odxQzu3JI/XHE8w3tlaVYnia7yA96HjUL71Ld+Dq7SW5/e2gvy3udVtdRbdFaoy1FpkEEfCDhmfPYlD85ZTf7W/Qzo0Jy/XZ/DuX01L6tEQVmxd1uA0D71bV+EhHqWF+S9R1e11Ft0UqhLxDSooHfOMWvlFh6Y7c3L2rtdU564bijn9T9O0/ZJZHwt1JfAtlVfD/U+F3j/dhgCzTsq1CWqGkTQO+d4O28bk2blsXzTHrpnZfDQ2BO4eFAHkhTwcrTKir1b7YZ2v2z7AlzAW5/Rxmud972oqvtFoS4+SPig/yB/O3+ZtYpPgvOy/vmqQVw+pKPmZZW6+2o5rHu/Kti3rwoJ9bZekPe9uKr7pXkHhbrEhYQN+kXrvHlZFxbspH2LNP7f5d68rKnJCnipo4oymPtb+PBR73nTdl6Q97+0qqXerL1CXeJWwgX90o27mTQ7jwV522jTrDH3XNKfscOySUvR/a/lKOxYA9Nu9FrxJ90Mw38Mzdv7XZVInSRM0K/bXsTv3lzJnM+9eVl/eWFfvntKV83LKkfHOfj0OXjzJ5CcCmP/7fW1i9RDCRP0jcxYunE3Pz2/D9ef1pWmmpdVjlbJXnjzTlj+InQdDpc/CS06+l2VyFFLmDTMbp3OB3eNUB+8HJvCXK+rZk8hnHs3nHGnpr2Tei9hgh5QyMvRCwTg/Qdh/u+hWQe48b/QeZjfVYlEREIFvchR2fslvHILrH0HBlwOFz8ITVr6XZVIxITVBDaz0Wa2yszyzeyuGtZnm9l8M1tiZsvM7MLg8lFmttjMlgf/PTfS34DIMVn1X3jidChcBJc+Clf9XSEvCeeILXozSwIeA0YBhcAiM5vunFsZstndwAvOucfNrD8wA+gKbAcucc5tNrOBwExAV7XEf+UlMOc38NETcNzxcOVUaNPb76pEoiKcrpthQL5zrgDAzJ4HxgChQe+Ag9MutQA2AzjnloRsswJoYmaNnXOlx1q4yFHbtgqmjYMty+GUH8DIeyC5sd9ViURNOEHfEdgY8rwQOLnaNvcAs8zsdiADGFnD61wJfFJTyJvZeGA8QHZ2dhgliRwF52DJs/DWzyGlCVz7AvQ+3++qRKIuUsNUrgGeds51Ai4EnjWzQ69tZgOAPwG31LSzc26ycy7HOZfTpk2bCJUkEuLAbnjxBph+O3Q6Cb7/gUJeGoxwWvSbgM4hzzsFl4UaB4wGcM59aGZpQBaw1cw6Aa8A33POrTn2kkXqaMNH8NJNsG+z101z2h3QSENxpeEI52xfBPQys25mlgqMBaZX22YDMALAzPoBacA2M2sJvAnc5Zx7P2JVi4QjUAnv3Ad/v8AL9htnwRkTFfLS4ByxRe+cqzCzCXgjZpKAqc65FWZ2L5DrnJsO/Bh4yswm4l2YvcE554L79QR+bWa/Dr7kec65rVH5bkQO2rMJXh4P69+D478FF90Pac2PvJ9IAjLnnN81HCYnJ8fl5ub6XYbUZ5+/AdMnQGW5F/CDx/pdkUjUmdli51xOTev0yVhJHOUHYNbdsGiKd5/4q6ZC6x5+VyXiOwW9JIYtK+GlcbB1JZx2O5z7a+/2wiKioJd6zjnInQozfwmNm8N1L0HPmj7GIdJwKeil/ire6Y2L/+INL9wvexyatvW7KpG4o6CX+mnde96omv1b4bzfe7cy0LBJkRop6KV+qayABffBgj9Dq25w0xxvcm4RqZWCXuqP3RvgpZth40I44TtwwX3QuKnfVYnEPQW91A8rXoHpd4ALwBVTYNDVflckUm8o6CW+lRXBf38BnzwDHXPgyimQ2c3vqkTqFQW9xK+vlnsTdW9f7U3Sfc4vISnF76pE6h0FvcQf5+Djyd6nXJtkwvdeg+5n+V2VSL2loJf4UrQDXrsN8t6C3qNhzF8ho7XfVYnUawp6iR8F73hj4w/s9EbUDBsPZn5XJVLvKejFf5XlMP//wXsPQFYvuG6aN2G3iESEgl78tXOtN/vTplwYej2M/gOkZvhdlUhCUdCLf5ZPg9d/5N264OpnYMBlflckkpAU9BJ7pfvhrZ/B0n9B51PgyqegZbbfVYkkLAW9xNbmpd7Y+F1r4ayfw5k/gySdhiLRpP9hEhuBACz8K8y5x7uV8PWvQ9cz/K5KpEFQ0Ev07d8Kr34f8udA34vh0kcgPdPvqkQaDAW9RFf+XHjlVijdCxdNgpwbNTZeJMYU9BIdFWUw71744BFo08+7jUG7/n5XJdIgKegl8nas8Sbq3rwEcsbB+b+HlCZ+VyXSYCnoJbI+fR7e/DE0SoZv/wv6Xex3RSINnoJeIqNkrxfwy1+ALqfDFZOhRSe/qxIRFPQSCYWL4aUbYfdGOOduGH4nNEryuyoRCVLQy9ELBOCDh2He/0Gz9vA/MyD7FL+rEpFqFPRydPZ9Ba/cAgVvQ//L4JIHoUkrn4sSkZoo6KXu8mbBq7dCWTFc8jAM/Z7GxovEsUbhbGRmo81slZnlm9ldNazPNrP5ZrbEzJaZ2YUh634R3G+VmZ0fyeIlxipK4a274N9XQ7MOcMs7cOL1CnmROHfEFr2ZJQGPAaOAQmCRmU13zq0M2exu4AXn3ONm1h+YAXQNPh4LDAA6AHPMrLdzrjLS34hE2bY874LrV8vh5Fth5G8hJc3vqkQkDOF03QwD8p1zBQBm9jwwBggNegc0Dz5uAWwOPh4DPO+cKwXWmll+8PU+jEDtEgvOwZJ/ercVTk6Da/4DfUb7XZWI1EE4Qd8R2BjyvBA4udo29wCzzOx2IAMYGbLvwmr7dqz+BmY2HhgPkJ2t+5LHjQO74Y2JsOJl6HYmXD4Zmrf3uyoRqaOw+ujDcA3wtHOuE3Ah8KyZhf3azrnJzrkc51xOmzZtIlSSHJMNH8ETw2HlazDiN/DdVxXyIvVUOC36TUDnkOedgstCjQNGAzjnPjSzNCArzH0lngQq4b1JMP8P3idbx82CTjl+VyUixyCcVvcioJeZdTOzVLyLq9OrbbMBGAFgZv2ANGBbcLuxZtbYzLoBvYCPI1W8RNieTfCPMTDvdzDgcrj1XYW8SAI4YoveOVdhZhOAmUASMNU5t8LM7gVynXPTgR8DT5nZRLwLszc45xywwsxewLtwWwHcphE3ceqLN+G127zbC1/2OAy+RsMmRRKEeXkcP3Jyclxubq7fZTQc5Qdg1v/Coqeg/WC4cipk9fS7KhGpIzNb7Jyr8U9wfTK2Idv6hTdR99YVcOoEGPFrSG7sd1UiEmEK+obIOVj8d/jvL6BxM/jOS9Br5JH3E5F6SUHf0BTvhNd/CJ+/Dj1GwOVPQNO2flclIlGkoG9I1r0PL98M+7fCeb+DU26DRpH6KIWIxCsFfUNQWQEL/gwL7oNWXb2x8R2H+l2ViMSIgj7R7d7oteI3fOgNmbzwz16/vIg0GAr6RLbyNZh+uzcT1BVPwaBv+V2RiPhAQZ+Iyoph5i9g8dPQ8US4cgpkdve7KhHxiYI+0Xz1mTc2fvsqOP1HcO7dkJTid1Ui4iMFfaJwDhZNgZm/giYtvbtN9jjH76pEJA4o6BNB0Q6YPgFWzYBe53n3qsnI8rsqEYkTCvr6bu0CeHk8FO+A0X/0pvnTzchEJISCvr6qLIe3/wDvToLWPeHaF6D9IL+rEpE4pKCvj3atg5dugsJFMOS7cMGfIDXD76pEJE4p6Oub5dO8eVwxuOrvMPAKvysSkTinoK8vyopgxs9g6T+h0zBvbHyrLn5XJSL1gIK+PvjyU29s/I41cOZP4ay7IEk/OhEJj9IinjkHCx+HOb+B9Cy4/nXoNtzvqkSknlHQx6v92+DV70P+bOhzEYx5FNIz/a5KROohBX08WjMPXr4FSvbAhX+Bk27S2HgROWoK+nhSUQbzfwfvPwRt+sL3XoV2A/yuSkTqOQV9vNhZANPGweZPIOdGOO/3kJrud1UikgAU9PHg0//Am3dCo2T41rPQ/1K/KxKRBKKg91PpPnjzJ7Dsecg+Da58Clp08rsqEUkwCnq/bFrsddXsXg9n/xLO/Ak0SvK7KhFJQAr6WAsE4MNHYO690PQ4uGEGdDnV76pEJIEp6GNp3xZ45RYomA/9LoVLH4YmrfyuSkQSnII+VlbPhldu9e5Zc8lDMPR6jY0XkZhQ0EdbRSnM+S0sfAzaDoCrpkLbvn5XJSINSKNwNjKz0Wa2yszyzeyuGtY/YGZLg195ZrY7ZN19ZrbCzD43s4fNGlAzdvtqmDLSC/lh4+HmeQp5EYm5I7bozSwJeAwYBRQCi8xsunNu5cFtnHMTQ7a/HRgSfHwacDpwcOqj94CzgLcjVH98cg6W/gtm/BSS02Dsc9D3Qr+rEpEGKpyum2FAvnOuAMDMngfGACtr2f4a4DfBxw5IA1IBA1KALcdScNwr2eNNDPLZS9B1OFwxGZp38LsqEWnAwgn6jsDGkOeFwMk1bWhmXYBuwDwA59yHZjYf+BIv6B91zn1ew37jgfEA2dnZdak/vmxcBC/dCHs2wbn/C2dM1Nh4EfFdWH30dTAWmOacqwQws55AP6AT3i+Mc83sazdUd85Nds7lOOdy2rRpE+GSYiBQCQv+AlPP957fOFMfgBKRuBFOi34T0DnkeafgspqMBW4LeX45sNA5tx/AzN4CTgXerXupcWrvZnh5PKx7FwZcAZc8CGkt/K5KROSQcFr0i4BeZtbNzFLxwnx69Y3MrC/QCvgwZPEG4CwzSzazFLwLsV/ruqm3Vr0Fj5/u3c5gzGPe0EmFvIjEmSO26J1zFWY2AZgJJAFTnXMrzOxeINc5dzD0xwLPO+dcyO7TgHOB5XgXZv/rnHs9ot+BH8pLYPb/wseT4bhBXsBn9fK7KhGRGtnhuey/nJwcl5ub63cZtdv6Bbw0DrZ8BqfcBiN/A8mN/a5KRBo4M1vsnMupaZ0+GRsu5+CTZ+CtuyA1A74zDXqN8rsqEZEjUtCH48AumP5D+Hw6dD8HLn8SmrXzuyoRkbAo6I9k/Yfw0k2w/ysYdS+cejs0ivSoVBGR6FHQ16ayAt79C7zzJ2jZBcbNgo4n+l2ViEidKehrsqcQXroZNnwAg8bCRX+Bxs38rkpE5Kgo6KtbOR2m3w6BCrh8Mgz+tt8ViYgcEwX9QWXFMPOXsPjv0GEIXPk3aN3D76pERI6Zgh5gywqYdiNs+wJOvwPOuRuSU/2uSkQkIhp20DsHi6bAzF95ty647mXoOcLvqkREIqrhBn3xTnhtAqx6E3qOgsseh6b18M6ZIiJH0DCDfu273h0ni7bB+X+Ak2/V2HgRSVgNK+grK+CdP3r3jm/dA66ZAx1O8LsqEZGoajhBv2u99wnXwo/hhOvggj9B46Z+VyUiEnUNI+g/ewlenwg4b9jk8Vf5XZGISMwkdtCXFcFbP4clz0Knk+DKKdCqq99ViYjEVOIG/ZfLvLHxO/Jh+E/g7LsgKcXvqkREYi7xgt45+OgJmP1rSG8N10+Hbmf6XZWIiG8SK+iLtsOrP4DVM6H3Bd48rhmt/a5KRMRXiRP0m5fAv78NB3bDBX+GYTeDmd9ViYj4LnGCvmUXaNsfzvsdHDfQ72pEROJG4gR9eiZ871W/qxARiTv63L+ISIJT0IuIJDgFvYhIglPQi4gkOAW9iEiCU9CLiCQ4Bb2ISIJT0IuIJDhzzvldw2HMbBuw/hheIgvYHqFyIkl11Y3qqhvVVTeJWFcX51yNE1/HXdAfKzPLdc7l+F1HdaqrblRX3aiuumlodanrRkQkwSnoRUQSXCIG/WS/C6iF6qob1VU3qqtuGlRdCddHLyIih0vEFr2IiIRQ0IuIJLh6E/RmNtrMVplZvpndVcP6xmb2n+D6j8ysa8i6XwSXrzKz82Nc151mttLMlpnZXDPrErKu0syWBr+mx7iuG8xsW8j73xSy7nozWx38uj7GdT0QUlOeme0OWRfN4zXVzLaa2We1rDczezhY9zIzGxqyLprH60h1fSdYz3Iz+8DMBoesWxdcvtTMcmNc19lmtifk5/XrkHXfeA5Eua6fhtT0WfCcygyui+bx6mxm84NZsMLM7qhhm+idY865uP8CkoA1QHcgFfgU6F9tmx8ATwQfjwX+E3zcP7h9Y6Bb8HWSYljXOUB68PH3D9YVfL7fx+N1A/BoDftmAgXBf1sFH7eKVV3Vtr8dmBrt4xV87TOBocBntay/EHgLMOAU4KNoH68w6zrt4PsBFxysK/h8HZDl0/E6G3jjWM+BSNdVbdtLgHkxOl7tgaHBx82AvBr+T0btHKsvLfphQL5zrsA5VwY8D4ypts0Y4Jng42nACDOz4PLnnXOlzrm1QH7w9WJSl3NuvnOuOPh0IdApQu99THV9g/OB2c65nc65XcBsYLRPdV0DPBeh9/5GzrkFwM5v2GQM8A/nWQi0NLP2RPd4HbEu59wHwfeF2J1f4Ryv2hzLuRnpumJ5fn3pnPsk+Hgf8DnQsdpmUTvH6kvQdwQ2hjwv5OsH6dA2zrkKYA/QOsx9o1lXqHF4v7EPSjOzXDNbaGaXRaimutR1ZfBPxGlm1rmO+0azLoJdXN2AeSGLo3W8wlFb7dE8XnVV/fxywCwzW2xm432o51Qz+9TM3jKzAcFlcXG8zCwdLyxfClkck+NlXrfyEOCjaquido4lzuTgcc7MrgNygLNCFndxzm0ys+7APDNb7pxbE6OSXgeec86VmtkteH8NnRuj9w7HWGCac64yZJmfxyuumdk5eEF/RsjiM4LHqy0w28y+CLZ4Y+ETvJ/XfjO7EHgV6BWj9w7HJcD7zrnQ1n/Uj5eZNcX75fIj59zeSL72N6kvLfpNQOeQ552Cy2rcxsySgRbAjjD3jWZdmNlI4FfApc650oPLnXObgv8WAG/j/ZaPSV3OuR0htUwBTgx332jWFWIs1f6sjuLxCkdttUfzeIXFzAbh/QzHOOd2HFwecry2Aq8QuS7LI3LO7XXO7Q8+ngGkmFkWcXC8gr7p/IrK8TKzFLyQ/5dz7uUaNoneORaNCw+R/sL7y6MA70/5gxdwBlTb5jYOvxj7QvDxAA6/GFtA5C7GhlPXELyLT72qLW8FNA4+zgJWE6GLUmHW1T7k8eXAQld14WdtsL5WwceZsaoruF1fvAtjFovjFfIeXan94uJFHH6h7ONoH68w68rGu+50WrXlGUCzkMcfAKNjWNdxB39+eIG5IXjswjoHolVXcH0LvH78jFgdr+D3/g/gwW/YJmrnWMQObrS/8K5I5+GF5q+Cy+7FayUDpAEvBk/6j4HuIfv+KrjfKuCCGNc1B9gCLA1+TQ8uPw1YHjzRlwPjYlzXH4AVwfefD/QN2ffG4HHMB/4nlnUFn98D/LHaftE+Xs8BXwLleH2g44BbgVuD6w14LFj3ciAnRsfrSHVNAXaFnF+5weXdg8fq0+DP+VcxrmtCyPm1kJBfRDWdA7GqK7jNDXgDNEL3i/bxOgPvGsCykJ/VhbE6x3QLBBGRBFdf+uhFROQoKehFRBKcgl5EJMEp6EVEEpyCXkQkwSnoRUQSnIJeRCTB/X9RktpQ7M9JAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,acc.shape[1],1), acc[1], label='val acc')\n",
    "plt.plot(np.arange(0,acc.shape[1],1), acc[0], label='train acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3db133ea60>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuC0lEQVR4nO3deXhU5dnH8e+dPYGQhSRsYRUQAUEgIAS07iIqCCgCUgFl19pqa1uX97Wv1tbutpYdEVwAUUFRq4gKVglbkEV2ArIEEQIJCZA9ed4/zgkMWWCAyZzJ5P5c11yZOUvmx8lwnzPnPM9zxBiDUkop/xXgdACllFLVSwu9Ukr5OS30Sinl57TQK6WUn9NCr5RSfi7I6QDlxcXFmRYtWjgdQymlapT169cfM8bEVzbP5wp9ixYtSE1NdTqGUkrVKCKyv6p5eupGKaX8nBZ6pZTyc1rolVLKz2mhV0opP6eFXiml/JwWeqWU8nNa6JVSys/5T6EvLYWlz8DxPU4nUUopn+I/hT5zL2x4A6b1gXWzQMfZV0opwJ8KfVxrmLgKmvWEj38Jbw6C7ENOp1JKKcf5T6EHiGoCIxbBnX+DA6thai/YvFCP7pVStZp/FXoAEeg+BiZ8A3FXwqKx8M5IOH3c6WRKKeUI/yv0ZepfAQ99Cjc/Bzv+A1N6ws5PnU6llFJe57+FHiAgEK57AsYth7oJMP9++OBRyM9xOplSSnmNfxf6Mg2vhrFfQp/HYeNbMK037PvG6VRKKeUVbhV6EekrIjtFJE1EflvFMkNEZJuIbBWReS7TS0Rko/1Y4qngFy0oFG75HYz+FCQQ5txltbsvyncsklJKecMFbzwiIoHAZOBWIB1YJyJLjDHbXJZpAzwF9DbGZIlIgsuvyDPGXOPZ2Jeh2bUwcSUs+19Y9W/YvQwGTYfGXZxOppRS1cKdI/oeQJoxZq8xphBYAAwot8xYYLIxJgvAGHPUszE9LKSO1QRzxHtQkAOzboEVf4KSIqeTKaWUx7lT6JsAB11ep9vTXLUF2orIShFZLSJ9XeaFiUiqPf2eyt5ARMbZy6RmZGRcTP7L0/oWmLQKOgyCFX+AV2+FjF3ee3+llPICT12MDQLaADcAw4CZIhJtz2tujEkChgMvi8gV5Vc2xswwxiQZY5Li4yu9t231CY+BwTPhvjmQtR+mXwerp1pj5yillB9wp9AfApq6vE60p7lKB5YYY4qMMd8Du7AKP8aYQ/bPvcAKwDdPhncYCJNWQ6sb4NPfwuv94cQBp1MppdRlc6fQrwPaiEhLEQkBhgLlW8+8j3U0j4jEYZ3K2SsiMSIS6jK9N7ANXxXZAIYtgP6vwA8bYEoybHhTh1BQStVoFyz0xphi4FFgKbAdWGiM2Soiz4tIf3uxpcBxEdkGLAeeNMYcB64CUkVkkz39JdfWOj5JBLo+aLXMadQJPngEFgyHU759fVkppaoixseOVpOSkkxqaqrTMSylpbB6CnzxPITWhbtehvb9L7iaUkp5m4ist6+HVlA7esZeqoAASH4Uxv8XohJh4U9h0XjIO+F0MqWUcpsWencktIMxX8BPfgPfvQNTk2HPcqdTKaWUW7TQuyswGG58GsYsg+AIeOMe+M+TUJjrdDKllDovLfQXq0k3mPA1XDsR1s6wbl14cJ3TqZRSqkpa6C9FcDjc8RKM/BBKCmH2bfDFC1Bc6HQypZSqQAv95Wh5vdUMs/Nw+PqvMOsmOLLV6VRKKXUOLfSXKywK7pkMQ+fByR9hxg2w8p9QWuJ0MqWUArTQe067O60hFNrcZg2BPOdOyNzrdCqllNJC71F14uD+N2HgdOsUztQ+kDpbh1BQSjlKC72niUDnodbwx027w0ePw1v3Qs5hp5MppWopLfTVJSoRRiyGfn+FfSthSk/47l2nUymlaiEt9NUpIAB6jIUJ30D91vDew/DOaMjNdDqZUqoW0ULvDXGt4aGlcNOzsH0JTOll3atWKaW8QAu9twQGwfVPwtgvISLWOm//4c+h4JTTyZRSfk4Lvbc16gxjl0PyY7B+rjVA2v4Up1MppfyYFnonBIfBbS/A6P9YrXRe6wefPQtF+U4nU0r5IS30TmqeDBNWQrdRkPKK1av28CanUyml/IwWeqeF1oW7X4YH3oW8LJh5E3z1FygpdjqZUspPaKH3FW1utTpZtR8Ay38Ps2+HY7udTqWU8gNa6H1JRCzcOxsGvwrH02DadbBmhnXvWqWUukRa6H3R1fdaA6S16AOfPGndzSo73elUSqkaSgu9r6rXCB54B+56GdJTrU5WG+frAGlKqYumhd6XiUDSaOvmJg06wPsT4O0RcPqY08mUUjWIFvqaILYljPoYbn0Bdn8Gk6+FHR87nUopVUNooa8pAgKh92Mw7ivrtM6C4fD+JMjPdjqZUsrHaaGvaRq0hzFfWuPmbJoPU3vD9/91OpVSyoe5VehFpK+I7BSRNBH5bRXLDBGRbSKyVUTmuUwfKSK77cdITwWv1YJCrJEwH/oMAkNg7t3wyW+gMNfpZEopH3TBQi8igcBk4A6gPTBMRNqXW6YN8BTQ2xjTAfiFPT0WeA64FugBPCciMZ78B7hKSTtGUUktanPetLs11n2P8bBmGky/HtLXO51KKeVj3Dmi7wGkGWP2GmMKgQXAgHLLjAUmG2OyAIwxR+3ptwPLjDGZ9rxlQF/PRD9X2tFTjHh1DWPmpnK6oBYNHxASAf3+DA9+AEW58Oqt8OWLUFLkdDKllI9wp9A3AQ66vE63p7lqC7QVkZUislpE+l7EuojIOBFJFZHUjIwM99O7aJ1QlxcHXs03ace4f8Yqjp6sZSNBtroBJqZApyHw3z/DrJvh6HanUymlfICnLsYGAW2AG4BhwEwRiXZ3ZWPMDGNMkjEmKT4+/pJDDOvRjFkPJrHn6GkGTk4h7ejJS/5dNVJ4NAycBve/afWknf4Ta1TM0hKnkymlHOROoT8ENHV5nWhPc5UOLDHGFBljvgd2YRV+d9b1qBvbJfD2+J4UFJcweOoq1n5fC+/PetXd1hAKrW+xxrmfezdk7XM6lVLKIe4U+nVAGxFpKSIhwFBgSbll3sc6mkdE4rBO5ewFlgK3iUiMfRH2NntateqUGM3iSb2pXzeEEa+u4ePNh6v7LX1P3QQY+hYMmAKHN1vNMNfP1SEUlKqFLljojTHFwKNYBXo7sNAYs1VEnheR/vZiS4HjIrINWA48aYw5bozJBF7A2lmsA563p1W7prERvDchmU5Nonhk3rfM+nqvN97Wt4hAlwdgUgo07gIfPgbz7oeTPzqdTCnlRWJ87AgvKSnJpKameuz35ReV8PjbG/lky4+M7t2CZ+9sT2CAeOz31xilpbB2Bnz+HASHw13/gA4DnU6llPIQEVlvjEmqbJ7f94wNCw5k8vCuPNynJa+t3Mcjb31LflEtvDgZEAA9J8D4ryGmJbwzCt4bY93VSinl1/y+0AMEBAj/c1d7nr3zKpZu+5EHZq0h63Sh07GcEd8WHl4GNz4DWxdbwx+nfe50KqVUNaoVhb7MmOtaMXl4V747lM3gqSkcOF5LhwwIDIKf/BrGfA5hUfDmYPjoCSg87XQypVQ1qFWFHqDf1Y14a8y1ZOYWMmjqSjann3A6knMad7FGw+z1KKTOtlrmHFjtdCqllIfVukIP0L1FLO9OSCYsOJD7p6/myx1HnI7knOAwuP1FGPURmBJ47Q5Y9hwUFzidTCnlIbWy0IM1ZMKiScm0TqjLmLmpzFtzwOlIzmrRxxpCocsIWPkyzLgRfvzO6VRKKQ+otYUeICEyjAXjenJ923ieXvwdf126E19rbupVoZHQ/xUYvhBOZ1jF/uu/QUktGiROKT9Uqws9QJ3QIGY9mMTQ7k359/I0frlwE4XFtWio48q0vd0aQqFdP/jieet0zvE9TqdSSl2iWl/oAYICA/jjoKv55a1tWbThEKPnrCUnv5YP81unPtw3FwbNgmM7YVofWDtTh1BQqgbSQm8TEX52cxv+el9n1uzNZMi0VRzOznM6lrNEoNN91tF9s57wn1/Bm4Mgu1rHpVNKeZgW+nLu7ZbIa6O7k56Vx6ApKez4McfpSM6r1xhGLII7/2Y1v5zSCzYv1KN7pWoILfSVuK5NPAvH96LUGO6buoqUtGNOR3KeCHQfY926MKEdLBoLCx+E08edTqaUugAt9FVo37geiyf1plF0GCNfW8v7G/R0BQD1r4DRn8Atv4Odn8CUntZPpZTP0kJ/Ho2jw3lnQjLdmsfwi7c3Mnl5Wu1uflkmIBD6PA7jVljj3s8fCh88Cvl6mkspX6SF/gKiwoOZ+1APBlzTmL8s3cmz72+huKSWN78s07AjjP0S+jwBG9+yhlDY943TqZRS5Wihd0NoUCD/GHINE2+4grfWHGDCm+vJLdRORAAEhcItz8HoT60j/Tl3wadPQ1Etb7GklA/RQu+mgADhN33b8cKADny54yjDZqzm2CkdD+aMZtfCxJXQ/WFYPdm6Mfmhb51OpZRCC/1F+2mvFkwb0Y2dR04yaEoK3x/ToX3PCKljNcEcsQgKTsKsW2DFS1BSyzufKeUwLfSX4LYODZk/tienCooZNGUl6/frXZrO0fpm6z61HQfDij/Cq7dCxk6nUylVa2mhv0RdmsWwaGIyUeHBDJ+5mk+36A23zxEeA4NnWsMoZO2H6dfDqinWvWuVUl6lhf4ytIirw3sTk7mqUT0mvrWeuSn7nI7kezrcYw2h0OoGWPoUvN4fTtTyIaGV8jIt9Jepft1Q5o/tyS1XNeC5JVv5w3+2U1qqbe3PEdkAhi2A/v+GHzbAlGTY8KYOoaCUl2ih94DwkECmjejGg72aM+O/e3lswQYKikucjuVbRKDrT62WOY06wwePwPxhcOqo08mU8nta6D0kMED4v/4deOqOdny0+TA/fXUt2bna2qSCmBYw8kO4/Q+w50trCIVtHzidSim/poXeg0SE8T+5gn8OvYaNB04weFoK6Vm5TsfyPQEB0OsRGP9fiGpqDY62aBzknXA6mVJ+SQt9NRhwTRNef7gHR3PyGTglhS2Hsp2O5JsS2sGYz+Env4Xv3oWpydZRvlLKo7TQV5Oererz7sRkggOE+6ev4qtdGU5H8k2BwXDjUzBmmdXh6o2B8PGvoFA7oinlKW4VehHpKyI7RSRNRH5byfxRIpIhIhvtxxiXeSUu05d4Mryva9sgksWP9KZZ/To8NGcdC1MPOh3JdzXpZp3K6TkJ1s2EadfBwXVOp1LKL1yw0ItIIDAZuANoDwwTkfaVLPq2MeYa+zHLZXqey/T+noldczSoF8bC8T1JvqI+v353My9/vkuHOq5KcDj0/aN1sbakEGbfZt2cvLjQ6WRK1WjuHNH3ANKMMXuNMYXAAmBA9cbyL5Fhwcwe1Z3BXRN5+fPd/Oa9zRTpUMdVa3k9TEyBzsPh67/BzJvgyFanUylVY7lT6JsArucc0u1p5Q0Wkc0i8q6INHWZHiYiqSKyWkTuqewNRGScvUxqRoZ/nssODgzgr/d14rGb27AwNZ2H56ZyqkCHOq5SWD24ZzIMnQ+nfoQZN8A3L0Op9k9Q6mJ56mLsh0ALY0wnYBkw12Vec2NMEjAceFlErii/sjFmhjEmyRiTFB8f76FIvkdEeOLWtvxp8NWsTDvG/dNXcTQn3+lYvq1dP2sIhba3w+fPwWv9IHOv06mUqlHcKfSHANcj9ER72hnGmOPGmLLB2WcB3VzmHbJ/7gVWAF0uI69fuL97M2aNTOL7Y6cZOCWFtKMnnY7k2+rEwZA3YOAMOLodpvaB1Nk6hIJSbnKn0K8D2ohISxEJAYYC57SeEZFGLi/7A9vt6TEiEmo/jwN6A9s8Ebymu/HKBN4e14uC4lIGTUlh7feZTkfybSLQ+X5r+OOm3eGjx+GteyHnB6eTKeXzLljojTHFwKPAUqwCvtAYs1VEnheRslY0j4nIVhHZBDwGjLKnXwWk2tOXAy8ZY7TQ265OjGLxpGTiIkMZMWsNH23WonVBUYkwYjH0+yvsWwlTelmdrZRSVRJfa+qXlJRkUlNTnY7hVSdyCxn7eirr9mXxTL+rGHNdS0TE6Vi+71gavD8B0tdBh4Fw598hItbpVEo5QkTW29dDK9CesT4gOiKENx6+ljuvbsSL/9nO/324jRId6vjC4lpbNyW/6X9g+0fWAGm7PnM6lVI+Rwu9jwgLDuSVYV0Y06clc1L2Memt9eQXaVPCCwoMgut/BWO/hIj6MO8+WPKYdc9apRSghd6nBAQIz97Vnv+9qz2fbTvC8JmryTytvULd0qgTjFsBvX8O374OU3vD/hSnUynlE7TQ+6CH+rRkyvCubPkhh8FTUzhwXIc6dktQKNz6PIz+xGql81o/+OxZKNK+Cqp200Lvo+64uhHzxlxLVm4hg6auZNPBE05Hqjma94IJK6HbKEh5xepV+8NGh0Mp5Rwt9D4sqUUs701MJjwkkKEzVvPF9iNOR6o5QuvC3S/DA+9CXhbMuhm++guU6LATqvbRQu/jroivy6KJvWmdUJexr6fy1pr9TkeqWdrcCpNWQft7YPnvrRExj+12OpVSXqWFvgaIjwxlwbie/KRtPM8s3sKfP92hQx1fjIhYuPdVuHe2NU7OtOtgzXQo1RFEVe2ghb6GqBMaxMwHkxjWoxlTVuzhiYWbKCzWQnVROg6GiaugRR/45Nfwxj2Qne50KqWqnRb6GiQoMIA/DOzIk7dfyeINhxg9Zy05+UVOx6pZ6jWCB96Bu/8J6anWEAob5+kAacqvaaGvYUSER25szd+HdGbN3kyGTFvF4ew8p2PVLCJWi5yJK6FBR3h/Irw9Ak75570QlNJCX0MN6prInNE9SM/KY+DkFHb8mON0pJontiWM+ghufQF2f2YNobD9I6dTKeVxWuhrsD5t4lg4vhcGw31TV7Ey7ZjTkWqegEDo/RiM+wrqNYa3H4DFEyE/2+lkSnmMFvoarn3jeiye1JvG0eGMem0tizfoxcVL0qA9jPkCrn8SNr8NU5Jh71dOp1LKI7TQ+4HG0eEsnNCLpOaxPP72JiYvT9Pml5ciKARuehYe/gyCw+D1/vDJb6BQh6BQNZsWej8RFR7MnIe6c881jfnL0p088/4Wiku0+eUlSUyC8V9Dj/GwZhpMvx7S1zudSqlLpoXej4QGBfL3Idcw6YYrmLfmAOPfWE9uoXb5vyQhEdDvz/DgB1CUB6/eCl/+XgdIUzWS3mHKT72xej/PfbCFq5tEMWtkd+IjQ52OVHPlZ8Mnv4VN80ACIbYVJFxlPeLbQUJ7qH8FBAY7nVTVYue7w5QWej+2bNsRfjb/W+IjQ5k7uget4us6Halm2/sV7Psajm63Hpl7Afv/T0AwxLWxC3/ZTuAqqwlnQKCjsVXtoIW+Ftt48AQPz1lHqTHMGplEt+Z6T1WPKcqDY7vg6A44ug0ydlg7gBMuA88FhkJ8W6voJ9hH//HtILo5BOiZU+U5Wuhruf3HTzNy9loOZ+fzz6Fd6NuxodOR/FvBKTi2s9wOYAfkuDR9DY6A+CvtHYDLaaCoRKvnrlIXSQu94vipAsa8nsrGgyd47q72jOrd0ulItU9+NmTsPHvqJ8P+ecrlPgOh9ewdgH30n9DO2hlENtQdgDovLfQKgLzCEn6+YAOfbTvC2Ota8tQdVxEQoMXDcbmZ9lH/NuvIv+x57vGzy4RFn3vuv+w0UJ04x2Ir36KFXp1RUmp4/sOtzF21nzs7NeJv93UmLFgvFvqkUxlnj/qPbj+7A3AdniEirmILoIR2EB7jXG7liPMV+iBvh1HOCgwQfte/A01iwvnDf3aQkVPAjAe7ER0R4nQ0VV7deOvR8vqz04yBkz+ee/H36HZrqOXCUy7rNqy4A4i/EsLqef/foRynR/S12JJNP/CrhZtoVj+COaO7kxgT4XQkdamMsW6icubcf9mF4J1Q7DKMdb1EewfQ7uyF4PgrIaSOc9mVR+ipG1Wl1XuPM+71VEKDA3ltVHc6NolyOpLypNJSOLHPPvdfdhpoh9UstKTAXkggpnnFJqBxba0xf1SNcNmFXkT6Av8EAoFZxpiXys0fBfwFOGRP+rcxZpY9byTwrD3998aYued7Ly303rf7yElGvbaOrNxCpjzQlRuuTHA6kqpuJcWQ9f255/6P7oDju6HUHjZDAqxewOU7gdVvbQ0Ap3zKZRV6EQkEdgG3AunAOmCYMWabyzKjgCRjzKPl1o0FUoEkrC6E64Fuxpisqt5PC70zjuTkM/q1dew8cpI/DryaId2bOh1JOaG4EDL3lGsCusOaZuxB8gKCrGJfvglobCsI1Mt+Trnci7E9gDRjzF77ly0ABgDbzruW5XZgmTEm0153GdAXmO9OcOU9DeqFsXBCLya+uZ5fv7eZQyfy+MUtbRBtu127BIWcPXp3VZRvHe27dgI7vAm2fcCZYSACQ6zTPed8A2gHMS10GAiHuVPomwAHXV6nA9dWstxgEbke6+j/cWPMwSrWbVJ+RREZB4wDaNasmXvJlcfVDQ1i9qjuPL3oO/75xW5+OJHHHwZdTXCgdtWv9YLDoOHV1sNVYW7FXsAH18KWd88uExRuDQNRdu6/bCcQ1VQ7gXmJp75nfQjMN8YUiMh4YC5wk7srG2NmADPAOnXjoUzqEgQHBvDnezvRODqcf36xmyMnC5jyQFfqhupXclWJkAho3MV6uCo4afcC3nb2QvDeFbDJ5ct8SF2rxc+ZTmD2I7KR7gA8zJ3/vYcA1xO2iZy96AqAMcalCx+zgD+7rHtDuXVXXGxI5V0iwuO3tqVxdBhPL97CkGmrmDO6Own1tAWGclNopHUDl8Ryp4zzsiruAHYthQ1vuqwbVbEJaMJVUCdedwCXyJ2LsUFYp2Nuxirc64DhxpitLss0MsYctp8PBH5jjOlpX4xdD3S1F/0W62JsZlXvpxdjfcuKnUeZ9Na3xESEMGd0d9o0iHQ6kvJHp4+f2wu47EJwnku7jfBYl4u/ZReCr4IIHZEVPNO8sh/wMlbzytnGmBdF5Hkg1RizRET+CPQHioFMYKIxZoe97kPA0/avetEY89r53ksLve/Zciib0XPWUVBUwowHk+jZqr7TkVRtYAycOlqxF3DGDijIObtc3QYVm4AmtIOw2tUnRDtMqct2MDOX0XPWceB4Ln8b0pm7Ozd2OpKqrYyBnB8q7wVcdPrscvWaVNwBxF8Jof55Ax4t9MojTuQWMu719azdl8nT/dox9rpW2vxS+Y7SUsg+UK4X8HarF3Cxy71+o5tVvA9A/JUQHO5cdg/QQq88Jr+ohF++s4mPNx9mZK/m/O/dHQjUoY6VLystgax9594DoGwYiNIiaxkJsNr7l98BxLWBoJpxv2UdvVJ5TFhwIK8M7ULjqDBmfv09h7Pz+dewLjrUsfJdAYHWzdvrXwFX3XV2ekmRdd/f8sNA7PoUTIm1jNjrlr8PQGyrGnUzeD2iV5fstZXf8/xH27imaTSvjuxObB0d/0T5geICOJ5W7j4AVdwMvvwOwMFewHrqRlWbT7cc5ucLNtI4Opw5o7vTvL4Od6v8lDs3gw8Ks4aBKH8jmKhm1X4zeC30qlqt35/JmLmpBIjw6qjuXNM02ulISnnPmZvBl/sGkOPSrzS4jksvYJcdQL0mHusEpoVeVbu9GacY+dpaMk4W8O9hXbmlfQOnIynlrDM3g992bkugCjeDb3f21E/DTtCi9yW9nRZ65RUZJwt4eO46thzK5vkBHRnRs7nTkZTyPeVvBl/WGij3OCR2hzGfX9Kv1VY3yiviI0NZMK4nP5u3gWff38KhE3k8eduVBGjzS6XOioiF5snWw9WpjHOHfPAgHX9WeVRESBDTf9qN4dc2Y+qKPTyxcCOFxaVOx1LK99WNt4ZzrgZ6RK88LigwgBfv6UiT6HD+snQnR3IKmPbTbkSF15x2x0r5Ez2iV9VCRHjkxtb84/7OpO7PZMi0VfxwIs/pWErVSlroVbUa2CWROaN78MOJPAZNSWH74ZwLr6SU8igt9Kra9W4dxzsTewEwZNoqVqYdcziRUrWLFnrlFe0a1mPxI8k0jg5n5Oy1LPo23elIStUaWuiV1zSKCuedib3o0TKWJxZuYvLyNHytH4dS/kgLvfKqemHBzBndg4FdmvCXpTt5evEWiku0+aVS1UmbVyqvCwkK4O9DOtM4OozJy/dwJCefV4Z1oU6ofhyVqg56RK8cISI8eXs7XhzYkRU7jzJs5moyThY4HUspv6SFXjnqgWubM/PBJHYfOcWgqSvZk3HK6UhK+R0t9MpxN1/VgAXjepJbUMLgqSmk7st0OpJSfkULvfIJnZtGs2hSMjERIQyftYZPvjvsdCSl/IYWeuUzmtevw3sTk+nYuB6T5n3L7G++dzqSUn5BC73yKbF1Qpg3tie3tW/A8x9t4/cfbaO0VNvaK3U5tNArnxMWHMiUB7oxKrkFs775np/N30B+UYnTsZSqsbThsvJJgQHCc3e3JzEmnN9/vJ2jJ/OZ+WAS0REhTkdTqsbRI3rls0SEMde14pVhXdh0MJvBU1M4mJnrdCylahy3Cr2I9BWRnSKSJiK/Pc9yg0XEiEiS/bqFiOSJyEb7Mc1TwVXtcXfnxrzxcA8yThYwaGoKWw5lOx1JqRrlgoVeRAKBycAdQHtgmIi0r2S5SODnwJpys/YYY66xHxM8kFnVQte2qs+iScmEBAYwZPoqlu886nQkpWoMd47oewBpxpi9xphCYAEwoJLlXgD+BOR7MJ9SZ7ROiGTxpGRaxtVhzNxU3l53wOlIStUI7hT6JsBBl9fp9rQzRKQr0NQY83El67cUkQ0i8pWIXFfZG4jIOBFJFZHUjIwMd7OrWiihXhhvj+9F79Zx/Oa97/j7sl061LFSF3DZF2NFJAD4O/DLSmYfBpoZY7oATwDzRKRe+YWMMTOMMUnGmKT4+PjLjaT8XN3QIF4dmcSQpET+9cVunnx3M0U61LFSVXKneeUhoKnL60R7WplIoCOwQkQAGgJLRKS/MSYVKAAwxqwXkT1AWyDVA9lVLRYcGMCfBneicXQ4L3++myM5+Ux5oCuRYcFOR1PK57hzRL8OaCMiLUUkBBgKLCmbaYzJNsbEGWNaGGNaAKuB/saYVBGJty/mIiKtgDbAXo//K1StJCL84pa2/PneTqTsOc6Q6as5kqOXiJQq74KF3hhTDDwKLAW2AwuNMVtF5HkR6X+B1a8HNovIRuBdYIIxRocmVB41JKkps0d158Dx0wycvJJdR046HUkpnyK+diErKSnJpKbqmR118bYcyuahOevIKyphxk+T6HVFfacjKeU1IrLeGJNU2TztGav8RscmUSyalEyDemGMnL2WJZt+cDqSUj5BC73yK4kxEbw3IZlrmkXz2PwNTP9qjza/VLWeFnrld6Iignnj4R7c1akRf/xkB88t2UqJDnWsajEdvVL5pdCgQP41tAtNosOZ/t+9HM7O519DuxAeEuh0NKW8To/old8KCBCe6ncV/9e/A59vP8Kwmas5fqrA6VhKeZ0WeuX3Ria3YOoD3dh+OIfBU1PYd+y005GU8iot9KpW6NuxIfPG9iQ7r4hBU1PYcCDL6UhKeY0WelVrdGsew3sTk6kbGsSwmatZtu2I05GU8got9KpWaRVfl0WTkrmyQSTj30jljVX7nI6kVLXTQq9qnbi6ocwf15Ob2iXwPx9s5aVPdlCqzS+VH9NCr2qliJAgpo3oxoiezZj21R4eX7iRguISp2MpVS20Hb2qtYICA3hhQEcaR4fz5093ciQnn+k/TSIqXIc6Vv5Fj+hVrSYiTLqhNS/ffw3r92dx37QUfjiR53QspTxKC71SwD1dmjB3dA8On8hn4JSVbPshx+lISnmMFnqlbMmt43hnYi8CRBgyfRXf7D7mdCSlPEILvVIu2jWsx6JJySTGhDPqtbW8tz7d6UhKXTYt9EqV0ygqnIUTenFtq1h++c4mXvlitw51rGo0LfRKVaJeWDCvjerBoC5N+NuyXTy9+DuKS0qdjqXUJdHmlUpVISQogL8N6Uzj6HD+vTyNH7PzeWlwJxIiQxERp+Mp5TYt9Eqdh4jwq9uvpHF0OM++/x3X/uELwoIDSIyJoGlMuPUz1v4ZE0FiTDjREcG6I1A+RQu9Um4Yfm0zOiVGsX5/Fgczc0nPyuNgVi7fHjhBdl7ROcvWDQ0i0d4JJMaE0zTW/hkTQWJsOPXCtEOW8i4t9Eq5qWOTKDo2iaowPSe/iPRMq/CnZ+Wd2RGkZ+Wyas8xTheeO7RCVHjw2cLvuiOwf0aE6H9L5Vn6iVLqMtULC6Z942DaN65XYZ4xhhO5RWe+AaRn5XIw09oJpGWcYsWuo+QXnXuRt36dEOsbges3AXtH0CQ6nLBgvR2iujha6JWqRiJCTJ0QYuqEcHVixW8DxhiOnSqs9NvAth9yWLb1CIXlWvskRIZWOCVU9rxxdDjBgdqYTp1LC71SDhIR4iNDiY8MpWuzmArzS0sNR08WVPg2cDAzj28PZPHR5sOUuAyxHCDQsF5Ypd8GEmPCaRQVTmCAXiiubbTQK+XDAgKEhlFhNIwKo3uL2Arzi0tK+TEn/+wOwP42kJ6Zx+o9x1mccwjXvl5BAUKj6LCzOwD7ArH1OoKEyFACdEfgd9wq9CLSF/gnEAjMMsa8VMVyg4F3ge7GmFR72lPAw0AJ8JgxZqkngiulrKGWE+0iDfUrzC8sLuVwdh4HMyteI1i+M4OMkwXnLB8SGECTmPAzrYbONh21fsbVDdGmozXQBQu9iAQCk4FbgXRgnYgsMcZsK7dcJPBzYI3LtPbAUKAD0Bj4XETaGmP0Dg9KeUFIUADN69ehef06lc7PLyo5c03A9dtAelYuS3/4kczThecsX1kfgrJvA01jw4kK1z4EvsidI/oeQJoxZi+AiCwABgDbyi33AvAn4EmXaQOABcaYAuB7EUmzf9+qyw2ulLp8YcGBtE6oS+uEupXOP11QfHZH4NJ/ID0rj/X7s8jJLz5nedc+BOW/DTSNDSdS+xA4wp1C3wQ46PI6HbjWdQER6Qo0NcZ8LCJPllt3dbl1m1xiVqWUl9UJDeLKhpFc2TCy0vnZeUXWt4ByLYYOZuaSsucYuZX0IWgaG05itMuOIPZs5zLtQ1A9LnurikgA8Hdg1GX8jnHAOIBmzZpdbiSllJdEhQcTFR5Fh8aVNx3Nyi06t7WQvVPYffQky3cepaC4kj4EVbQY0j4El86dQn8IaOryOtGeViYS6AissM/NNQSWiEh/N9YFwBgzA5gBkJSUpOPBKuUHRITYOiHE1gmhU2J0hfnGGDJOFVT4NpCelcfWQ9l8tvVHikrOLQcJkaHnDinhsiPQPgRVc6fQrwPaiEhLrCI9FBheNtMYkw3Elb0WkRXAr4wxqSKSB8wTkb9jXYxtA6z1XHylVE0lIiREhpEQGVZlH4IjJ/PP2RGU/Vy///x9CCobXqJhvbBa24fggoXeGFMsIo8CS7GaV842xmwVkeeBVGPMkvOsu1VEFmJduC0GHtEWN0opdwQECI2irE5eVfUhOJydf+Z0ULrLxeKUPcf4MSe/Qh+CxtHhVY4zFF/Xf/sQiK/dOScpKcmkpqY6HUMpVcMVFJdw+ER+heElyl5X6EMQFEBidDhNyo84ar+uX8e3+xCIyHpjTFJl8/QSt1LKL4UGBdIirg4t4s7fh6CybwRbt1TsQxAeHGg3HS2/I/D9PgRa6JVStdKF+hCcKijm0JlvAmc7lB3MzCN1fxYny/UhiAwNqvLbQGKMs30ItNArpVQl6rrRh6B8a6GDmbkcOJ7LyrSKfQiiI6q4D0FMBE2quQ+BFnqllLoEUeHBRFVxM5qyPgTnXhewvg3sOnKSL3dU7EMQVzeEnq3q8+/hXT2eVQu9Ukp5mGsfgs5NoyvMLy01HDtdcKYjWdm3gtg6IdWSRwu9Ukp5WUDA2T4E3ZpX7EPg8fer9ndQSinlKC30Sinl57TQK6WUn9NCr5RSfk4LvVJK+Tkt9Eop5ee00CullJ/TQq+UUn7O54YpFpEMYP9l/Io44JiH4niS5ro4muviaK6L44+5mhtj4iub4XOF/nKJSGpVYzI7SXNdHM11cTTXxaltufTUjVJK+Tkt9Eop5ef8sdDPcDpAFTTXxdFcF0dzXZxalcvvztErpZQ6lz8e0SullHKhhV4ppfxcjSn0ItJXRHaKSJqI/LaS+aEi8rY9f42ItHCZ95Q9faeI3O7lXE+IyDYR2SwiX4hIc5d5JSKy0X4s8XKuUSKS4fL+Y1zmjRSR3fZjpJdz/cMl0y4ROeEyrzq312wROSoiW6qYLyLyLzv3ZhHp6jKvOrfXhXI9YOf5TkRSRKSzy7x99vSNIpLq5Vw3iEi2y9/rf13mnfczUM25nnTJtMX+TMXa86pzezUVkeV2LdgqIj+vZJnq+4wZY3z+AQQCe4BWQAiwCWhfbplJwDT7+VDgbft5e3v5UKCl/XsCvZjrRiDCfj6xLJf9+pSD22sU8O9K1o0F9to/Y+znMd7KVW75nwGzq3t72b/7eqArsKWK+f2ATwABegJrqnt7uZkruez9gDvKctmv9wFxDm2vG4CPLvcz4Olc5Za9G/jSS9urEdDVfh4J7Krk/2S1fcZqyhF9DyDNGLPXGFMILAAGlFtmADDXfv4ucLOIiD19gTGmwBjzPZBm/z6v5DLGLDfG5NovVwOJHnrvy8p1HrcDy4wxmcaYLGAZ0NehXMOA+R567/MyxvwXyDzPIgOA141lNRAtIo2o3u11wVzGmBT7fcF7ny93tldVLuez6elc3vx8HTbGfGs/PwlsB5qUW6zaPmM1pdA3AQ66vE6n4kY6s4wxphjIBuq7uW515nL1MNYeu0yYiKSKyGoRucdDmS4m12D7K+K7ItL0ItetzlzYp7haAl+6TK6u7eWOqrJX5/a6WOU/Xwb4TETWi8g4B/L0EpFNIvKJiHSwp/nE9hKRCKxi+Z7LZK9sL7FOK3cB1pSbVW2fMb05uJeIyAggCfiJy+TmxphDItIK+FJEvjPG7PFSpA+B+caYAhEZj/Vt6CYvvbc7hgLvGmNKXKY5ub18mojciFXo+7hM7mNvrwRgmYjssI94veFbrL/XKRHpB7wPtPHSe7vjbmClMcb16L/at5eI1MXaufzCGJPjyd99PjXliP4Q0NTldaI9rdJlRCQIiAKOu7ludeZCRG4BngH6G2MKyqYbYw7ZP/cCK7D28l7JZYw57pJlFtDN3XWrM5eLoZT7Wl2N28sdVWWvzu3lFhHphPU3HGCMOV423WV7HQUW47lTlhdkjMkxxpyyn/8HCBaROHxge9nO9/mqlu0lIsFYRf4tY8yiShapvs9YdVx48PQD65vHXqyv8mUXcDqUW+YRzr0Yu9B+3oFzL8buxXMXY93J1QXr4lObctNjgFD7eRywGw9dlHIzVyOX5wOB1ebshZ/v7Xwx9vNYb+Wyl2uHdWFMvLG9XN6jBVVfXLyTcy+Ura3u7eVmrmZY152Sy02vA0S6PE8B+noxV8Oyvx9WwTxgbzu3PgPVlcueH4V1Hr+Ot7aX/W9/HXj5PMtU22fMYxu3uh9YV6R3YRXNZ+xpz2MdJQOEAe/YH/q1QCuXdZ+x19sJ3OHlXJ8DR4CN9mOJPT0Z+M7+oH8HPOzlXH8Ettrvvxxo57LuQ/Z2TANGezOX/fp3wEvl1qvu7TUfOAwUYZ0DfRiYAEyw5wsw2c79HZDkpe11oVyzgCyXz1eqPb2Vva022X/nZ7yc61GXz9dqXHZElX0GvJXLXmYUVgMN1/Wqe3v1wboGsNnlb9XPW58xHQJBKaX8XE05R6+UUuoSaaFXSik/p4VeKaX8nBZ6pZTyc1rolVLKz2mhV0opP6eFXiml/Nz/A8MJRJO9lLaGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,loss.shape[1],1), loss[1], label='val acc')\n",
    "plt.plot(np.arange(0,loss.shape[1],1), loss[0], label='train acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH='/media/sergio/traballos sergio/msi/models/cassava/VGG16-2.h5'\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/media/sergio/traballos sergio/msi/models/cassava/VGG16-2-best.h5'\n",
    "torch.save(best_model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "images: 1000\n",
      "0 Loss: 0.4861 Acc: 0.8330\n",
      "1 Loss: 0.3932 Acc: 0.8623\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "images: 1000\n",
      "0 Loss: 0.4753 Acc: 0.8360\n",
      "1 Loss: 0.4009 Acc: 0.8617\n",
      "\n",
      "Training complete in 27m 41s\n",
      "Best val Acc: 0.862305\n",
      "train loss: [0.4860530502452251, 0.47531284963301507]\n",
      "\n",
      "val loss: [0.3931747988661362, 0.4008864563196387]\n",
      "\n",
      "train acc: [0.8329576070819816, 0.8359817452026173]\n",
      "\n",
      "val acc: [0.8623052959501558, 0.8616822429906542]\n"
     ]
    }
   ],
   "source": [
    "model, loss, acc = train_model(model, dataloader, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transformations for data augmentation\n",
    "jitter_param = 0.3\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(448),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=jitter_param,\n",
    "            contrast=jitter_param,\n",
    "            saturation=jitter_param),\n",
    "        transforms.RandomRotation(degrees=(-30,30), \n",
    "                                  resample=False, \n",
    "                                  expand=False, \n",
    "                                  center=None, \n",
    "                                  fill=None),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(448),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#We define the dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, imgs, labels, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        img = Image.open(self.imgs[ix])\n",
    "        label = torch.tensor(self.labels[ix], dtype = torch.long)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    \n",
    "    def check(self):\n",
    "        if len(self.img) == len(self.labels):\n",
    "            print('Okey!')\n",
    "        else:\n",
    "            print('imgs and labels different len')\n",
    "\n",
    "#Take train data and create a dataset class with it\n",
    "database_base_path = '/media/sergio/traballos sergio/msi/datasets/kaggle/cassava/'\n",
    "df = pd.read_csv(database_base_path + 'train.csv')\n",
    "#imgs = [database_base_path + 'train_images/' + img for img in os.listdir(database_base_path)]\n",
    "\n",
    "#Shuffle data\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "#Take imgs and labels\n",
    "imgs = [database_base_path + 'train_images/' + img for img in df['image_id'].values]\n",
    "labels = df['label'].values\n",
    "\n",
    "train_val_split = 0.85\n",
    "batch_size = 6\n",
    "\n",
    "train_dataset = Dataset(imgs[0:700], \n",
    "                        labels[0:700], \n",
    "                        train_transform)\n",
    "val_dataset = Dataset(imgs[1000:1500], \n",
    "                      labels[1000:1500], \n",
    "                      val_transform)\n",
    "\n",
    "#create the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=True, num_workers=4)\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                                shuffle=False, num_workers=4)\n",
    "\n",
    "dataloader = []\n",
    "dataloader.append(train_loader)\n",
    "dataloader.append(validation_loader)\n",
    "\n",
    "dataset_sizes = []\n",
    "dataset_sizes.append(len(train_dataset))\n",
    "dataset_sizes.append(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.0001, momentum=0.75)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "0 Loss: 1.4239 Acc: 0.4914\n",
      "1 Loss: 1.1418 Acc: 0.6260\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "0 Loss: 1.2096 Acc: 0.5743\n",
      "1 Loss: 1.0764 Acc: 0.6260\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "0 Loss: 1.1282 Acc: 0.5800\n",
      "1 Loss: 1.0208 Acc: 0.6500\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-61fb4791be31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_ft, loss, acc = train_model(model, dataloader, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        num_epochs=5)\n",
      "\u001b[0;32m<ipython-input-3-c9f910933221>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft, loss, acc = train_model(model, dataloader, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last acc: [0.87507] resnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
